{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMot102vH5jWJvXxiWSfPm6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReyhaneTaj/ML_Algorithms/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression: Concept and Implementation\n",
        "\n",
        "## What is Logistic Regression?\n",
        "Logistic Regression is a supervised machine learning model used for binary classification tasks. It predicts the probability of a binary outcome (1/0, Yes/No, True/False) based on one or more predictor variables.\n",
        "\n",
        "- **Model Structure**: Logistic regression uses a logistic function to model a binary dependent variable.\n",
        "- **Equation**: The logistic function (sigmoid) is defined as \\( P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} \\), where:\n",
        "  - \\( P(y=1|x) \\) is the probability of the target variable being 1.\n",
        "  - \\( x \\) is the predictor variable.\n",
        "  - \\( \\beta_0 \\) is the intercept.\n",
        "  - \\( \\beta_1 \\) is the coefficient for the predictor variable.\n",
        "\n",
        "## Assumptions of Logistic Regression\n",
        "1. **Binary Outcome**: The dependent variable is binary.\n",
        "2. **Linearity of Logits**: The log-odds of the outcome are a linear combination of the predictor variables.\n",
        "3. **Independence**: Observations are independent of each other.\n",
        "4. **No Multicollinearity**: Predictors are not highly correlated with each other.\n",
        "\n",
        "## How Logistic Regression Works\n",
        "1. **Model Training**: The model learns the best-fitting parameters by maximizing the likelihood of the observed data.\n",
        "2. **Prediction**: New data is passed through the logistic function to produce a probability. The outcome is determined by a threshold (usually 0.5).\n",
        "\n",
        "## Advantages\n",
        "- **Simplicity**: Easy to understand and implement.\n",
        "- **Interpretability**: Coefficients provide clear insights into the relationship between predictors and the probability of the target outcome.\n",
        "- **Probabilistic Output**: Provides probabilities, which can be useful for decision-making.\n",
        "\n",
        "## Disadvantages\n",
        "- **Linear Boundaries**: Assumes a linear decision boundary, which may not hold in all cases.\n",
        "- **Outliers**: Sensitive to outliers, which can disproportionately influence the model.\n",
        "- **Multicollinearity**: Can be problematic if predictors are highly correlated.\n",
        "\n",
        "## Implementation in Google Colab\n",
        "Let's implement a Logistic Regression model using the Breast Cancer dataset.\n",
        "\n",
        "### Code Implementation\n"
      ],
      "metadata": {
        "id": "aUuXxCV0-4xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Split Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Logistic Regression Model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cbtERetu--Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison with Other Models**\n",
        "\n",
        "**Binary Classification:** Logistic regression is well-suited for binary classification but can be extended to multiclass classification through techniques like one-vs-rest or multinomial logistic regression.\n",
        "\n",
        "**Interpretability:** Logistic regression is highly interpretable, with coefficients directly indicating the relationship between predictors and the log-odds of the outcome.\n",
        "\n",
        "**Complexity:** Less complex compared to models like Decision Trees or Neural Networks, but may be less accurate for non-linear relationships.\n",
        "\n",
        "**Conclusion**\n",
        "Logistic Regression is a powerful and interpretable algorithm for binary classification tasks. Its simplicity and probabilistic output make it a valuable tool for many applications, although it may be limited by assumptions of linearity and sensitivity to outliers."
      ],
      "metadata": {
        "id": "iUDqh3sz-_7E"
      }
    }
  ]
}