{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzrW+s33nXtZAgvWyr+Ko+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReyhaneTaj/ML_Algorithms/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost (eXtreme Gradient Boosting)\n",
        "\n",
        "XGBoost is a powerful and efficient implementation of the gradient boosting algorithm, widely used for regression, classification, and ranking problems. It is known for its speed, performance, and flexibility.\n",
        "\n",
        "## Key Features of XGBoost\n",
        "\n",
        "### 1. Gradient Boosting Framework\n",
        "XGBoost is built on the gradient boosting framework, where models are trained sequentially to correct errors made by previous models. Each model in the sequence improves the overall performance by learning from the residuals (errors) of the preceding models.\n",
        "\n",
        "### 2. Regularization\n",
        "XGBoost includes L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting and improve the model's generalization to unseen data. This makes XGBoost more robust compared to other gradient boosting implementations.\n",
        "\n",
        "### 3. Parallel Processing\n",
        "One of the standout features of XGBoost is its ability to parallelize the training process, significantly reducing training time. This is achieved by parallelizing the tree construction process, making it faster than traditional gradient boosting methods.\n",
        "\n",
        "### 4. Handling Missing Values\n",
        "XGBoost has a built-in mechanism to handle missing data, allowing the model to learn patterns even when some features are missing. This is particularly useful in real-world scenarios where datasets may have missing or incomplete information.\n",
        "\n",
        "### 5. Tree Pruning\n",
        "XGBoost employs a technique called \"max depth pruning,\" which stops growing a tree when additional splits do not improve model performance. This helps in reducing overfitting and speeds up the computation.\n",
        "\n",
        "### 6. Cross-Validation and Early Stopping\n",
        "XGBoost includes built-in cross-validation and early stopping features. Cross-validation helps in evaluating model performance, while early stopping prevents overfitting by halting the training process when the model's performance stops improving.\n",
        "\n",
        "### 7. Scalability\n",
        "XGBoost is highly scalable and can be distributed across a cluster of machines, making it suitable for training on large datasets.\n",
        "\n",
        "## Applications of XGBoost\n",
        "- **Classification Tasks:** Predicting binary or multi-class outcomes, such as spam detection or customer churn prediction.\n",
        "- **Regression Tasks:** Estimating continuous values, like predicting house prices or stock market trends.\n",
        "- **Ranking Problems:** Used in recommendation systems and search engines to rank items or pages.\n",
        "\n",
        "## Step-by-Step Process of XGBoost\n",
        "\n",
        "### 1. Data Preprocessing\n",
        "- **Collect Data:** Start by gathering and organizing the dataset.\n",
        "- **Handle Missing Values:** XGBoost can internally handle missing values, but preprocessing might still be required.\n",
        "- **Feature Engineering:** Create and modify features to improve model performance.\n",
        "- **Split Data:** Divide the data into training and testing (or validation) sets.\n",
        "\n",
        "### 2. Model Initialization\n",
        "- **Define Objective Function:** Choose an objective function (e.g., `binary:logistic` for classification or `reg:squarederror` for regression).\n",
        "- **Set Hyperparameters:** Configure model parameters like `n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `lambda`, and `alpha`.\n",
        "\n",
        "### 3. Boosting Process\n",
        "- **Initialize Base Model:** Start with an initial prediction (e.g., mean value for regression).\n",
        "- **Compute Pseudo-Residuals:** Calculate residuals between observed and predicted values.\n",
        "- **Fit a New Tree on Residuals:** Train a tree to predict these residuals.\n",
        "- **Update Model:** Combine the new tree's predictions with previous ones.\n",
        "- **Repeat:** Continue this process for the specified number of boosting rounds or until early stopping criteria are met.\n",
        "\n",
        "### 4. Regularization and Pruning\n",
        "- **Apply Regularization:** Use L1 or L2 regularization to prevent overfitting.\n",
        "- **Tree Pruning:** Control splits using parameters like `max_depth` and `gamma` to avoid unnecessary complexity.\n",
        "\n",
        "### 5. Prediction\n",
        "- **Make Predictions:** Use the trained model to predict outcomes on the test set.\n",
        "- **Thresholding:** For classification tasks, apply a threshold to convert probabilities into class labels.\n",
        "\n",
        "### 6. Model Evaluation\n",
        "- **Evaluate Performance:** Assess the model using metrics like Accuracy, Precision, Recall, F1-Score, Mean Squared Error (MSE), or Area Under the Curve (AUC).\n",
        "- **Cross-Validation:** Perform cross-validation to obtain a robust estimate of model performance.\n",
        "\n",
        "### 7. Tuning and Optimization\n",
        "- **Hyperparameter Tuning:** Optimize hyperparameters using techniques like grid search or random search.\n",
        "- **Early Stopping:** Monitor validation performance and stop training when performance plateaus.\n",
        "\n",
        "### 8. Deployment\n",
        "- **Final Model Training:** Retrain the model on the entire dataset with the best hyperparameters.\n",
        "- **Deploy the Model:** Implement the model in a production environment for making real-time predictions.\n",
        "\n",
        "### 9. Monitoring and Maintenance\n",
        "- **Monitor Model Performance:** Continuously track the model's performance in production to detect any degradation.\n",
        "- **Retraining:** Regularly retrain the model as new data becomes available to ensure it remains accurate and reliable.\n",
        "\n",
        "In this example, we'll use XGBoost to predict house prices using the California Housing dataset. This is a regression problem where the target variable is the median house value in different California districts."
      ],
      "metadata": {
        "id": "UXNR5k4T2l6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost for California Housing Price Prediction\n",
        "\n",
        "# Step 1: Import Libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Load and Preprocess the Data\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the XGBoost Model\n",
        "\n",
        "# Initialize the XGBoost regressor\n",
        "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=4, learning_rate=0.1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make Predictions and Evaluate the Model\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f'RMSE: {rmse:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG3_8ksN2r5I",
        "outputId": "bbadde4a-6d96-4b13-a38b-9f356f05ec1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "An RMSE (Root Mean Squared Error) of **0.51** indicates that, on average, the predictions made by the XGBoost model differ from the actual house prices by about 0.51 units of the target variable.\n",
        "\n",
        "Given that the California Housing dataset's target variable represents the median house value in units of $100,000, this RMSE translates to an average prediction error of about **$51,000**.\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- **Low RMSE**: A lower RMSE value indicates a better fit of the model to the data, meaning the predictions are closer to the actual values.\n",
        "- **High RMSE**: Conversely, a higher RMSE suggests that the model’s predictions are less accurate.\n",
        "\n",
        "In this context, an RMSE of 0.51 is a reasonable result, but whether it is \"good\" depends on the specific use case and acceptable error margin in predicting house prices.\n",
        "\n",
        "### Improving the Model\n",
        "\n",
        "If you want to reduce the RMSE further, consider the following:\n",
        "\n",
        "- **Hyperparameter Tuning**: Adjust hyperparameters such as `max_depth`, `learning_rate`, `n_estimators`, `subsample`, etc.\n",
        "- **Feature Engineering**: Add, remove, or transform features to better capture the underlying patterns.\n",
        "- **Cross-Validation**: Use cross-validation to ensure the model generalizes well to unseen data.\n",
        "- **Regularization**: Experiment with the regularization parameters (`alpha`, `lambda`) to prevent overfitting.\n",
        "\n",
        "The goal is to balance model complexity and performance to achieve the best possible prediction accuracy.\n"
      ],
      "metadata": {
        "id": "8umQIbljBAg-"
      }
    }
  ]
}
