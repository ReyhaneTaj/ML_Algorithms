{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx94aTWZ/NHnrz5/TfTs3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReyhaneTaj/ML_Algorithms/blob/main/Feature_Scaling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling with Standardization and Normalization\n",
        "\n",
        "When working with data that contains features with different units or scales (e.g., age in years, income in dollars), some machine learning algorithms can perform poorly if these features are not scaled. This is because many algorithms (like KNN, SVM, and gradient descent-based models) are sensitive to the magnitude of the features.\n",
        "\n",
        "### 1. Standardization (Z-score normalization)\n",
        "\n",
        "Standardization rescales data so that it has a mean of 0 and a standard deviation of 1. This is useful for algorithms that assume Gaussian distribution of the data or when dealing with features that have different units.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Standardized value} = \\frac{X - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(X\\) is the original value.\n",
        "- \\(\\mu\\) is the mean of the feature.\n",
        "- \\(\\sigma\\) is the standard deviation of the feature.\n",
        "\n",
        "### 2. Normalization (Min-Max Scaling)\n",
        "\n",
        "Normalization rescales the feature values to a fixed range, usually [0, 1]. This is useful when you need the data to be bounded within a certain range.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Normalized value} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(X\\) is the original value.\n",
        "- \\(X_{\\text{min}}\\) is the minimum value of the feature.\n",
        "- \\(X_{\\text{max}}\\) is the maximum value of the feature.\n",
        "\n",
        "### Practical Example in Python\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = np.array([[1, 200], [2, 300], [3, 400]])\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)\n",
        "print(\"Standardized Data:\\n\", standardized_data)\n",
        "\n",
        "# Normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "normalized_data = min_max_scaler.fit_transform(data)\n",
        "print(\"Normalized Data:\\n\", normalized_data)\n"
      ],
      "metadata": {
        "id": "A796WwLK7j2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scaling and Data Leakage\n",
        "\n",
        "Data scaling can lead to data leakage if not handled properly, particularly when the scaling process includes information from the test set during the training process. Data leakage occurs when information from outside the training dataset is used to create the model, causing the model to perform well on the test data but fail in real-world scenarios.\n",
        "\n",
        "### How Data Scaling Can Cause Data Leakage\n",
        "\n",
        "#### Improper Scaling Across Entire Dataset\n",
        "\n",
        "If you scale your entire dataset (including both training and test sets) before splitting into training and test sets, the scaler will incorporate information from the test data, which can lead to overfitting. This is because the scaling parameters (mean, standard deviation, min, max, etc.) will be influenced by the test data, allowing the model to \"see\" the test data indirectly.\n",
        "\n",
        "#### Example\n",
        "\n",
        "Suppose you have a dataset that you want to standardize. If you calculate the mean and standard deviation of the entire dataset (including both the training and test sets) and then scale the data, your test set scaling will be biased. This means that the model has already been exposed to some information about the test set during training, leading to overly optimistic performance estimates.\n",
        "\n",
        "### Correct Way to Scale Data\n",
        "\n",
        "#### Fit on Training Data Only\n",
        "\n",
        "Fit the scaler (whether itâ€™s a StandardScaler, MinMaxScaler, or another method) only on the training data. This ensures that the scaling parameters are derived solely from the training data, preventing any information from the test data from leaking into the training process.\n",
        "\n",
        "#### Apply Scaling Separately\n",
        "\n",
        "After fitting the scaler on the training data, apply the same transformation to both the training and test data. This means that the test data is scaled using the parameters learned from the training data only.\n"
      ],
      "metadata": {
        "id": "r7Cz48ooEPFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Practical Example in Python\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = np.array([[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]])\n",
        "labels = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the scaler and fit on the training data only\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply the same transformation to the test data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Now X_train_scaled and X_test_scaled are properly scaled without any data leakage.\n"
      ],
      "metadata": {
        "id": "7umlw71-7qWb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **Always** fit the scaler on the training data only.\n",
        "- **Never** include test data in the fitting process of any preprocessing steps to avoid data leakage.\n",
        "- Apply the fitted transformation consistently to both training and test sets.\n",
        "\n",
        "By following these steps, you can prevent data leakage and ensure that your model's performance estimates are realistic.\n"
      ],
      "metadata": {
        "id": "0kG_IqrtEnPe"
      }
    }
  ]
}